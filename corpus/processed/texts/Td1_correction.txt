Solution: Propagation Avant et R´etropropagation dans
un MLP Binaire
4 mai 2025
1
Propagation Avant (Forward Pass)
Donn´ees :
— Entr´ee X =
 0.7
−0.5

— Poids couche 1 : W(1) =
0.2
−0.3
0.4
0.1

— Biais couche 1 : b(1) =
 0.1
−0.2

— Poids couche 2 : W(2) =

0.5
−0.6

— Biais couche 2 : b(2) = −0.1
1.1
Calcul de la sortie de la couche cach´ee :
z(1) = W(1)X + b(1)
z(1) =
0.2
−0.3
0.4
0.1
  0.7
−0.5

+
 0.1
−0.2

z(1)
1
= 0.2 × 0.7 + (−0.3) × (−0.5) + 0.1 = 0.14 + 0.15 + 0.1 = 0.39
z(1)
2
= 0.4 × 0.7 + 0.1 × (−0.5) + (−0.2) = 0.28 −0.05 −0.2 = 0.03
Donc z(1) =
0.39
0.03

Appliquons la fonction d’activation sigmo¨ıde : σ(z) =
1
1+e−z
a(1) = σ(z(1))
a(1)
1
= σ(0.39) =
1
1+e−0.39 =
1
1+0.677 =
1
1.677 ≈0.596
a(1)
2
= σ(0.03) =
1
1+e−0.03 =
1
1+0.97 =
1
1.97 ≈0.507
Donc a(1) =
0.596
0.507

1.2
Calcul de la sortie finale :
z(2) = W(2)a(1) + b(2)
z(2) =

0.5
−0.6
 0.596
0.507

+ (−0.1)
z(2) = 0.5 × 0.596 + (−0.6) × 0.507 + (−0.1)
z(2) = 0.298 −0.304 −0.1 = −0.106
1

Appliquons la fonction d’activation sigmo¨ıde :
a(2) = σ(z(2)) = σ(−0.106) =
1
1+e0.106 =
1
1.112 ≈0.474
La sortie finale du r´eseau est donc ˆy = 0.474
2
Calcul de la Fonction de Perte (Entropie crois´ee
binaire)
L’entropie crois´ee binaire est donn´ee par : L = −[y log(ˆy) + (1 −y) log(1 −ˆy)]
Avec y = 1 et ˆy = 0.474 :
L = −[1 × log(0.474) + (1 −1) × log(1 −0.474)]
L = −[log(0.474) + 0]
L = −log(0.474) = −(−0.747) = 0.747
La valeur de la perte est donc L = 0.747
3
R´etropropagation (Backward Pass)
3.1
Gradient de la couche de sortie :
∂L
∂z(2) = a(2) −y = 0.474 −1 = −0.526
3.2
Gradients des poids et biais de la couche 2 :
∂L
∂W(2) =
∂L
∂z(2) · (a(1))T
∂L
∂W(2) = −0.526 ×

0.596
0.507

∂L
∂W (2)
1
= −0.526 × 0.596 = −0.313
∂L
∂W (2)
2
= −0.526 × 0.507 = −0.267
∂L
∂b(2) =
∂L
∂z(2) = −0.526
3.3
Gradient de la sortie de la couche cach´ee :
∂L
∂a(1) = (W(2))T ·
∂L
∂z(2)
∂L
∂a(1) =
 0.5
−0.6

× (−0.526)
∂L
∂a(1)
1
= 0.5 × (−0.526) = −0.263
∂L
∂a(1)
2
= −0.6 × (−0.526) = 0.316
3.4
Gradient de l’entr´ee de la couche cach´ee :
La d´eriv´ee de la fonction sigmo¨ıde est σ′(z) = σ(z) × (1 −σ(z))
∂L
∂z(1) =
∂L
∂a(1) ⊙σ′(z(1))
∂L
∂z(1) =
∂L
∂a(1) ⊙(a(1) ⊙(1 −a(1)))
∂L
∂z(1)
1
= −0.263 × 0.596 × (1 −0.596) = −0.263 × 0.596 × 0.404 = −0.063
∂L
∂z(1)
2
= 0.316 × 0.507 × (1 −0.507) = 0.316 × 0.507 × 0.493 = 0.079
2

3.5
Gradients des poids et biais de la couche 1 :
∂L
∂W(1) =
∂L
∂z(1) · XT
∂L
∂W (1)
11 = −0.063 × 0.7 = −0.044
∂L
∂W (1)
12 = −0.063 × (−0.5) = 0.032
∂L
∂W (1)
21 = 0.079 × 0.7 = 0.055
∂L
∂W (1)
22 = 0.079 × (−0.5) = −0.040
∂L
∂b(1) =
∂L
∂z(1)
∂L
∂b(1)
1
= −0.063
∂L
∂b(1)
2
= 0.079
4
Mise `a jour des poids et biais (α = 0.01)
4.1
Mise `a jour des poids et biais de la couche 2 :
W(2)
new = W(2) −α ·
∂L
∂W(2)
W (2)
1
new = 0.5 −0.01 × (−0.313) = 0.5 + 0.00313 = 0.503
W (2)
2
new = −0.6 −0.01 × (−0.267) = −0.6 + 0.00267 = −0.597
b(2)
new = b(2) −α ·
∂L
∂b(2)
b(2)
new = −0.1 −0.01 × (−0.526) = −0.1 + 0.00526 = −0.095
4.2
Mise `a jour des poids et biais de la couche 1 :
W(1)
new = W(1) −α ·
∂L
∂W(1)
W (1)
11 new = 0.2 −0.01 × (−0.044) = 0.2 + 0.00044 = 0.200
W (1)
12 new = −0.3 −0.01 × 0.032 = −0.3 −0.00032 = −0.300
W (1)
21 new = 0.4 −0.01 × 0.055 = 0.4 −0.00055 = 0.399
W (1)
22 new = 0.1 −0.01 × (−0.040) = 0.1 + 0.00040 = 0.100
b(1)
new = b(1) −α ·
∂L
∂b(1)
b(1)
1
new = 0.1 −0.01 × (−0.063) = 0.1 + 0.00063 = 0.101
b(1)
2
new = −0.2 −0.01 × 0.079 = −0.2 −0.00079 = −0.201
5
R´esum´e des r´esultats
5.1
Propagation avant :
— Sortie de la couche cach´ee : a(1) =
0.596
0.507

— Sortie finale : ˆy = 0.474
5.2
Fonction de perte :
— L = 0.747
3

5.3
Gradients :
—
∂L
∂W(2) =

−0.313
−0.267

—
∂L
∂b(2) = −0.526
—
∂L
∂W(1) =
−0.044
0.032
0.055
−0.040

—
∂L
∂b(1) =
−0.063
0.079

5.4
Poids et biais mis `a jour :
— W(2)
new =

0.503
−0.597

— b(2)
new = −0.095
— W(1)
new =
0.200
−0.300
0.399
0.100

— b(1)
new =
 0.101
−0.201

4
